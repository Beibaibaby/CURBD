{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c523dd65-6d03-4128-8669-527cd9cb731c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Multi-region data-constrained recurrent neural network (RNN) models and perform current-based decomposition (CURBD)\n",
    "\n",
    "We are now ready to implement the CURBD from scratch.\n",
    "In particular, we will train this RNN to function as a surrogate brain.\n",
    "See the orignial code at: https://github.com/rajanlab/CURBD/blob/master/curbd.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "190326d0-5c8a-4008-8a20-c7759818b735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a23fcb-25aa-4d09-8564-2ee4e16f5177",
   "metadata": {},
   "source": [
    "### Here is a list of parameters in the original codes    \n",
    "    activity: numpy.array\n",
    "        N X T\n",
    "    dtData: float\n",
    "        time step (in s) of the training data\n",
    "    dtFactor: float\n",
    "        number of interpolation steps for RNN\n",
    "    g: float\n",
    "        instability (chaos); g<1=damped, g>1=chaotic\n",
    "    tauRNN: float\n",
    "        decay constant of RNN units\n",
    "    tauWN: float\n",
    "        decay constant on filtered white noise inputs\n",
    "    ampInWN: float\n",
    "        input amplitude of filtered white noise\n",
    "    nRunTrain: int\n",
    "        number of training runs\n",
    "    nRunFree: int\n",
    "        number of untrained runs at end\n",
    "    P0: float\n",
    "        learning rate\n",
    "    nonLinearity: function\n",
    "        inline function for nonLinearity\n",
    "    resetPoints: list of int\n",
    "        list of indeces into T. default to only set initial state at time 1.\n",
    "    plotStatus: bool\n",
    "        whether to plot data fits during training\n",
    "    verbose: bool\n",
    "        whether to print status updates\n",
    "    regions: dict()\n",
    "        keys are region names, values are np.array of indeces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0341620c-2b0a-47f1-be9a-7ffe2f8f9624",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 0.1 #instability (chaos); g<1=damped, g>1=chaotic\n",
    "num_neurons = 20 #Should based on data\n",
    "num_time = 22 #Should based on data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e8acfb-e301-4a4b-9185-7eeb4706d127",
   "metadata": {},
   "source": [
    "We use some beta data for development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08ecb90c-e4b6-48c6-b538-0c760e5ede9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.random.rand(num_neurons,num_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f9406c3-6299-43ee-b0c9-11c00e8e7b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, time_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim  # Number of hidden dimensions\n",
    "        self.layer_dim = layer_dim    # Number of hidden layers, here the default is one\n",
    "        \n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')  # RNN\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)  # Here use a linear layer as Readout layer\n",
    "        \n",
    "        # Initialize hidden-to-hidden weight to random values\n",
    "        for l in range(layer_dim):\n",
    "            # Get the attribute name for the hidden-to-hidden weight for the layer 'l'\n",
    "            weight_name = f\"weight_hh_l{l}\"\n",
    "            getattr(self.rnn, weight_name).data.uniform_(-g, g)\n",
    "\n",
    "    def forward(self, x, k=1):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
    "        \n",
    "        outputs = []\n",
    "        for _ in range(time_dim):\n",
    "            # Perform one step of the RNN\n",
    "            out, h0 = self.rnn(x, h0)\n",
    "            # Process the RNN output through the FC layer\n",
    "            out = self.fc(out[:, -1, :])\n",
    "            outputs.append(out)\n",
    "            # Use the current output as the next input (May need to reshape it properly)\n",
    "            x = out.unsqueeze(1)\n",
    "        \n",
    "        # Stack outputs to have them in [batch, k, output_dim] shape\n",
    "        return torch.stack(outputs, dim=1)\n",
    "\n",
    "\n",
    "# batch_size, epoch and iteration\n",
    "#batch_size = 100\n",
    "#n_iters = 8000\n",
    "#num_epochs = n_iters / (len(features_train) / batch_size)\n",
    "#num_epochs = int(num_epochs)\n",
    "\n",
    "# Pytorch train and test sets\n",
    "#train = TensorDataset(featuresTrain,targetsTrain)\n",
    "#test = TensorDataset(featuresTest,targetsTest)\n",
    "\n",
    "# data loader\n",
    "#train_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\n",
    "#test_loader = DataLoader(test, batch_size = batch_size, shuffle = False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb786d3a-8225-498b-875a-069b6608af15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Generate multiple sine waves with different frequencies as an example time series\n",
    "time_steps = np.linspace(0, 10, 1000)\n",
    "data = np.array([np.sin(time_steps * (i * 0.1 + 1)) for i in range(20)]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dd1157-22e6-4684-95a0-319e8a589bd8",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "For simplicity, let's assume each input sequence to the RNN consists of 10 values, and we want to predict the next 10 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc409d62-1fd6-4286-bb11-5d91b34f186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = num_time\n",
    "train_data = []\n",
    "train_labels = []\n",
    "\n",
    "for i in range(len(data) - seq_length*2):\n",
    "    train_data.append(data[i:i+seq_length])\n",
    "    train_labels.append(data[i+seq_length:i+2*seq_length])\n",
    "\n",
    "# Convert lists of numpy arrays to numpy arrays\n",
    "train_data_np = np.array(train_data)\n",
    "train_labels_np = np.array(train_labels)\n",
    "\n",
    "# Convert numpy arrays to tensors\n",
    "train_data = torch.tensor(train_data_np, dtype=torch.float32)\n",
    "train_labels = torch.tensor(train_labels_np, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c713121-9a36-49bc-91aa-0efda1e9c745",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c7ba976-7094-4ed1-b0ff-81959535ff7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 0.5275\n",
      "Epoch [11/200], Loss: 0.5135\n",
      "Epoch [21/200], Loss: 0.5036\n",
      "Epoch [31/200], Loss: 0.4756\n",
      "Epoch [41/200], Loss: 0.4510\n",
      "Epoch [51/200], Loss: 0.4264\n",
      "Epoch [61/200], Loss: 0.3828\n",
      "Epoch [71/200], Loss: 0.3480\n",
      "Epoch [81/200], Loss: 0.3193\n",
      "Epoch [91/200], Loss: 0.2870\n",
      "Epoch [101/200], Loss: 0.2493\n",
      "Epoch [111/200], Loss: 0.2204\n",
      "Epoch [121/200], Loss: 0.1984\n",
      "Epoch [131/200], Loss: 0.1802\n",
      "Epoch [141/200], Loss: 0.1650\n",
      "Epoch [151/200], Loss: 0.1451\n",
      "Epoch [161/200], Loss: 0.1303\n",
      "Epoch [171/200], Loss: 0.1186\n",
      "Epoch [181/200], Loss: 0.1084\n",
      "Epoch [191/200], Loss: 0.0976\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = num_neurons\n",
    "hidden_dim = 32\n",
    "layer_dim = 1\n",
    "output_dim = num_neurons\n",
    "learning_rate = 0.001\n",
    "epochs = 200\n",
    "time_dim = num_time \n",
    "\n",
    "model = RNNModel(input_dim, hidden_dim, layer_dim, output_dim, time_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs = model(train_data)\n",
    "    loss = criterion(outputs, train_labels)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133190e0-e964-4224-808d-b95d3e893616",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
